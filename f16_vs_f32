import torch
import time
from torch.cuda.amp import autocast

# Taille des matrices
M, N, K = 4096, 4096, 4096

# Force l'utilisation de la précision élevée pour les opérations FP32
torch.set_float32_matmul_precision('high')

# Génère des matrices aléatoires en FP32 sur le GPU
A_fp32 = torch.rand(M, K, device='cuda', dtype=torch.float32)
B_fp32 = torch.rand(K, N, device='cuda', dtype=torch.float32)

# Test FP32
torch.cuda.synchronize()
start_fp32 = time.time()
C_fp32 = torch.matmul(A_fp32, B_fp32)
torch.cuda.synchronize()
end_fp32 = time.time()

time_fp32 = end_fp32 - start_fp32
flops_fp32 = 2 * M * N * K / time_fp32

# Convertit les matrices en FP16
A_fp16 = A_fp32.to(dtype=torch.float16)
B_fp16 = B_fp32.to(dtype=torch.float16)

# Test FP16 avec autocast (mixed precision)
torch.cuda.synchronize()
start_fp16 = time.time()
with autocast(dtype=torch.float16):
    C_fp16 = torch.matmul(A_fp16, B_fp16)
torch.cuda.synchronize()
end_fp16 = time.time()

time_fp16 = end_fp16 - start_fp16
flops_fp16 = 2 * M * N * K / time_fp16

# Affiche les performances
print(f"FP32 : {time_fp32:.4f} s | {flops_fp32:.2e} FLOPS")
print(f"FP16 : {time_fp16:.4f} s | {flops_fp16:.2e} FLOPS")

# Calcule l'erreur relative
C_fp16_upcast = C_fp16.to(dtype=torch.float32)
relative_error = (C_fp32 - C_fp16_upcast).abs().mean() / C_fp32.abs().mean()
print(f"Erreur relative moyenne FP16 vs FP32 : {relative_error:.2e}")
