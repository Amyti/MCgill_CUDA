import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import time
from tqdm import tqdm


app = int(input("1 pour utiliser CUDA et autre pour utiliser le CPU : "))


device = torch.device("cuda" if torch.cuda.is_available() and app ==1 else "cpu")
print(f'utilisation de : {device}')


transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
trainloader = DataLoader(trainset, batch_size=41096, shuffle=True)

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.flatten(x)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

def entrainer(optimiseur_type,nom, epochs=10):
    model = MLP().to(device)
    loss_fn = nn.CrossEntropyLoss()
    optimiseur = optimiseur_type(model.parameters(), lr=0.08)
    pertes = []

    for _ in tqdm(range(epochs), desc=f"apprentissage avec l'optimiseur {nom}"):
        model.train()
        perte_totale = 0

        for images, labels in trainloader:
            images = images.to(device)
            labels = labels.to(device)
            pred = model(images)
            perte = loss_fn(pred, labels)

            optimiseur.zero_grad()
            perte.backward()
            optimiseur.step()

            perte_totale += perte.item()
        pertes.append(perte_totale / len(trainloader))
    
    return pertes

types_optimiseurs = {
    "SGD": optim.SGD,
    "Adam": optim.Adam,
    "ASGD": optim.ASGD
}

resultats = {}
torch.cuda.synchronize()
debut = time.time()

for nom, opt in types_optimiseurs.items():
    resultats[nom] = entrainer(opt, nom)

torch.cuda.synchronize()
fin = time.time()

print(f'durée total : {f"{(fin-debut)} secondes" if (fin-debut) < 60 else f"{((fin-debut)/60)} minutes" }')

plt.figure(figsize=(10, 6))
for nom, pertes in resultats.items():
    plt.plot(pertes, label=nom)
plt.xlabel("Époques")
plt.ylabel("Perte moyenne")
plt.title("Comparaison des optimiseurs")
plt.legend()
plt.grid(True)
plt.show()

Thu May  1 12:21:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3080        Off |   00000000:01:00.0  On |                  N/A |
|  0%   61C    P2            108W /  340W |    1069MiB /  10240MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A             751      G   /usr/bin/gnome-shell                    125MiB |
|    0   N/A  N/A            1097      G   /usr/bin/Xwayland                         8MiB |
|    0   N/A  N/A            1770    C+G   /usr/bin/kgx                             46MiB |
|    0   N/A  N/A            2640      G   /usr/lib/firefox/firefox                154MiB |
|    0   N/A  N/A            3271    C+G   /usr/bin/nautilus                        22MiB |
|    0   N/A  N/A            3365      G   ...ess --variations-seed-version         44MiB |
|    0   N/A  N/A            3655    C+G   tecla                                    23MiB |
|    0   N/A  N/A            4687      C   python                                  546MiB |
+-----------------------------------------------------------------------------------------+
