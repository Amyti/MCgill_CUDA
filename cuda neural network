
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("üñ•Ô∏è Appareil utilis√© :", device)

t64 = torch.randn((2, 2), dtype=torch.float64)
print("\nD√©mo r√©duction de pr√©cision :")
print("Float64 :", t64.dtype)

t32 = t64.float()
print("Float32 :", t32.dtype)

t16 = t32.half()
print("Float16 :", t16.dtype)

X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)
X = torch.tensor(X, dtype=torch.float32).to(device)
y = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(device)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(2, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

net = MLP().to(device)

# passer tout en float16  :
# net = net.half()
# X_train = X_train.half()
# y_train = y_train.half()
# X_test = X_test.half()
# y_test = y_test.half()

criterion = nn.BCELoss()  
optimizer = optim.SGD(net.parameters(), lr=0.01)

losses = []
for epoch in range(200):
    net.train()
    y_pred = net(X_train)
    loss = criterion(y_pred, y_train)
    losses.append(loss.item())

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 20 == 0:
        print(f"√âpoque {epoch} | Perte : {loss.item():.4f}")

net.eval()
with torch.no_grad():
    predictions = net(X_test) >= 0.5
    accuracy = (predictions == y_test).float().mean()
    print(f"\nüéØ Pr√©cision sur test : {accuracy.item()*100:.2f}%")

plt.plot(losses)
plt.title("Courbe de perte (Loss)")
plt.xlabel("√âpoque")
plt.ylabel("Loss")
plt.grid()
plt.show()


üéØ Pr√©cision sur test : 88.50%
/home/amir/Proj_Neur/cuda.py:78: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()
(neuralnetwork) [amir@amyti Proj_Neur]$ python cuda.py 
üñ•Ô∏è Appareil utilis√© : cuda

D√©mo r√©duction de pr√©cision :
Float64 : torch.float64
Float32 : torch.float32
Float16 : torch.float16
√âpoque 0 | Perte : 0.6792
√âpoque 20 | Perte : 0.6758
√âpoque 40 | Perte : 0.6723
√âpoque 60 | Perte : 0.6687
√âpoque 80 | Perte : 0.6650
√âpoque 100 | Perte : 0.6612
√âpoque 120 | Perte : 0.6572
√âpoque 140 | Perte : 0.6532
√âpoque 160 | Perte : 0.6489
√âpoque 180 | Perte : 0.6446

üéØ Pr√©cision sur test : 70.00%
/home/amir/Proj_Neur/cuda.py:78: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()
(neuralnetwork) [amir@amyti Proj_Neur]$ python cuda.py 
üñ•Ô∏è Appareil utilis√© : cuda

D√©mo r√©duction de pr√©cision :
Float64 : torch.float64
Float32 : torch.float32
Float16 : torch.float16
√âpoque 0 | Perte : 0.7806
√âpoque 20 | Perte : 0.7547
√âpoque 40 | Perte : 0.7310
√âpoque 60 | Perte : 0.7089
√âpoque 80 | Perte : 0.6882
√âpoque 100 | Perte : 0.6686
√âpoque 120 | Perte : 0.6499
√âpoque 140 | Perte : 0.6321
√âpoque 160 | Perte : 0.6150
√âpoque 180 | Perte : 0.5986

üéØ Pr√©cision sur test : 85.00%
/home/amir/Proj_Neur/cuda.py:78: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()
